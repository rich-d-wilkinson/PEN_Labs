{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past Earth Network Emulator Workshop: Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on ones written by [Alan Saul](http://www.alansaul.com/), James Hensman, Neil Lawrence, and Nicolas Durrande. In it, we will run through some of the basic features of GPy. We will focus on three aspects of GPs: the kernel/covariance function, how to generate random sample paths, and how to do GP regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that GPy is already installed on your machine. You can get instructions on how to install GPy from the  [SheffieldML github page](https://github.com/SheffieldML/GPy). The online documentation for GPy is available from [this page](http://gpy.readthedocs.org/en/latest/).\n",
    "\n",
    "To start off, we must first tell the Jupyer notebook that we want the plots to appear inline and import the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Covariance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance function is the single most important aspect of a GP. We can use any function we like as long as it is positive semi-definite. We usually build a suitable covariance function by combining a small number of well-known covariance functions. However, it is possible to add your own kernels to GPy without too much trouble. A good source of information on covariance functions is available in the [kernel cookbook](http://www.cs.toronto.edu/~duvenaud/cookbook/index.html)\n",
    "\n",
    "Let's start by defining an exponentiated quadratic covariance function (also known as squared exponential or rbf - radial basis function -  or Gaussian) in one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 1       # input dimension\n",
    "var = 1.       # variance\n",
    "theta = 0.2    # lengthscale\n",
    "k = GPy.kern.RBF(d, variance=var, lengthscale=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the kernel can be obtained using the command `print k`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot the kernel as a function of one of its inputs (whilst fixing the other) with `k.plot()`. \n",
    "\n",
    "*Note*: if you need help with a command in ipython notebook, then you can get it at any time by typing a question mark after the command, e.g. `k.plot?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Covariance Function Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the covariance function parameters can be accessed and modified using `k['.*var']` where the string in brackets is a regular expression matching the parameter name as it appears in `print k`. You can also access each parameter directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(k.lengthscale)\n",
    "k['.*lengthscale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to get an insight into the effect of the parameters on the shape of the covariance function, by setting the lengthscale of the covariance function to different values, and plotting the resulting covariance using the `k.plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(d)     # By default, the parameters are set to 1.\n",
    "theta = np.asarray([0.2,0.5,1.,2.,4.])\n",
    "for t in theta:\n",
    "    k.lengthscale=t\n",
    "    k.plot()\n",
    "plt.legend(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the effect of the lengthscale parameter on the covariance function?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exercise 1 a) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now change the code above to investigate the effect of changing the variance parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1 b) answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Functions in GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many different covariance functions are already implemented in GPy. Instead of rbf, try constructing and plotting the following  covariance functions: `exponential`, `Matern32`, `Matern52`, `Brownian`, `linear`, `bias`,\n",
    "`rbfcos`, `periodic_Matern32`, etc. Some of these covariance functions, such as `rbfcos`, are not\n",
    "parametrized by a variance and a lengthscale. Furthermore, not all kernels are stationary (i.e., they can’t all be written as $k ( x, y) = k ( x − y)$, see for example the Brownian\n",
    "covariance function). For plotting  so it may be interesting to change the value of the fixed input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kb = GPy.kern.Brownian(input_dim=1)\n",
    "inputs = np.array([2., 4.])\n",
    "for x in inputs:\n",
    "    kb.plot(x,plot_limits=[0,5])\n",
    "plt.legend(inputs)\n",
    "plt.ylim(-0.1,5.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Plot some other covariance functions. As we will see, it is very hard to intuit from the shape of these what the resulting GP sample paths will look like. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the covariance/Gram matrix given the input data, $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{X}$ be a $n$ × $d$ numpy array. Given a kernel $k$, the covariance matrix associated to\n",
    "$\\mathbf{X}$ is obtained with `C = k.K(X,X)` . The positive semi-definiteness of $k$ ensures that `C`\n",
    "is a positive semi-definite (psd) matrix regardless of the initial points $\\mathbf{X}$. This can be\n",
    "checked numerically by looking at the eigenvalues (they should all be positive if C is positive definite):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = GPy.kern.Matern52(input_dim=2)\n",
    "X = np.random.rand(50,2)       # 50*2 matrix of iid standard Gaussians\n",
    "C = k.K(X,X)\n",
    "eigvals = np.linalg.eigvals(C)           # Computes the eigenvalues of a matrix\n",
    "plt.bar(np.arange(len(eigvals)), eigvals)\n",
    "plt.title('Eigenvalues of the Matern 5/2 Covariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prior Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian processes can be used as infinite dimensional models of function. They are defined by a covariance *function* and a mean *function*. \n",
    "\n",
    "When we compute the covariance matrix using `kern.K(X, X)` we are computing a covariance *matrix* between the values of the function, $f$, that correspond to the input locations in the matrix `X`. If we want to have a look at the type of functions that arise from a particular Gaussian process, we can never generate all possible values, but we can generate random samples from a Gaussian *distribution* based on a covariance matrix associated with a particular matrix of input locations `X`. If these locations are chosen appropriately then they give us a good idea of the underlying function. For example, for a one dimensional function, if we choose `X` to be uniformly spaced across part of the real line, and the spacing is small enough, we'll get an idea of the underlying function. We will now use this trick to draw sample paths from a Gaussian process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(input_dim=1,lengthscale=0.2)\n",
    "\n",
    "X = np.linspace(0.,1.,500) # define X to be 500 points evenly spaced over [0,1]\n",
    "X = X[:,None] # reshape X to make it n*p \n",
    "\n",
    "mu = np.zeros((500)) # vector of the means --- we could use a mean function, but here it is just zero.\n",
    "C = k.K(X,X) # compute the covariance matrix associated with inputs X\n",
    "\n",
    "# Generate 20 separate samples paths from a Gaussian with mean mu and covariance C\n",
    "Z = np.random.multivariate_normal(mu,C,20)\n",
    "\n",
    "plt.figure()     # open a new plotting window\n",
    "for i in range(20):\n",
    "    plt.plot(X[:],Z[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of `X` means that the points are close enough together to look like functions. We can see the structure of the covariance matrix we are plotting from if we visualize C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: \n",
    "Try a range of different covariance functions and hyper-parameter values and plot the corresponding sample paths for each using the same approach given above. Compare these to the shapes of the covariance functions you found in Exercise 2. Can you see any relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try plotting sample paths here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample paths from a GP inherit their properties (such as continuity, differentiability, smoothness etc) from the particular covariance function used.\n",
    "\n",
    "Can you tell the covariance structures \n",
    "that have been used for generating the\n",
    "sample paths shown in the figure below?\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figa.png\" alt=\"Figure a\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figb.png\" alt=\"Figure b\" style=\"width: 30%;\">\n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figc.png\" alt=\"Figure c\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figd.png\" alt=\"Figure d\" style=\"width: 30%;\">\n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/fige.png\" alt=\"Figure e\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figf.png\" alt=\"Figure f\" style=\"width: 30%;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exercise 4 answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A Gaussian Process Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now combine the Gaussian process prior with some data to form a GP regression model with GPy. We will generate data from the function $f ( x ) = 0.3\\cos(1.3 x ) + \\sin(0.3x )$ over $[0, 10]$, adding some noise to give $y(x) = f(x) + \\epsilon$, with the noise being Gaussian distributed, $\\epsilon \\sim \\mathcal{N}(0, 0.01)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.uniform(0, 10, (200, 1))\n",
    "f = np.sin(.3*X) + .3*np.cos(1.3*X)\n",
    "Y = f+np.random.normal(0, .1, f.shape)\n",
    "plt.scatter(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GP regression model based on an exponentiated quadratic covariance function can be defined by first defining a covariance function, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then combining it with the data to form a Gaussian process model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = GPy.models.GPRegression(X,Y,k)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as for the covariance function object, we can find out about the model using the command `print m`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default the model includes some observation noise\n",
    "with variance 1. We can see the posterior mean prediction and visualize the marginal posterior variances using `m.plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual predictions of the model for a set of points `Xstar`\n",
    "(an $m \\times p$ array) can be computed using \n",
    "\n",
    "`Ystar, Vstar = m.predict(Xstar)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xp = np.linspace(-2,12)[:,None]\n",
    "Ystar, Vstar = m.predict(Xp, full_cov=True)\n",
    "up95, lo95 = m.predict_quantiles(Xp, quantiles=(2.5,97.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the inputs to GPy need to be 2-d arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xp = np.linspace(-2,12)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What do you think about this first fit? Does the prior given by the GP seem to be\n",
    "adapted?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Exercise 4 a) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) The parameters of the models can be modified using a regular expression matching the parameters names (for example `m['.*noise'] = 0.001` ), or by `m.Gaussian_noise.variance=0.001`. Change the values of the parameters to obtain a better fit, replotting your fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 4 b) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) As in Section 2, random sample paths from the conditional GP can be obtained using\n",
    "`np.random.multivariate_normal(mu[:,0],C)` where the mean vector and covariance\n",
    "matrix `mu` and `C` are obtained through the predict function `mu, C = m.predict(Xp,full_cov=True)`. Obtain 10 samples from the posterior sample and plot them alongside the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 4 c) answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `GPy.models` commands are wrappers for a collection of commands. `GPRegression` makes two default choices here:\n",
    "\n",
    "1. Specifies a Gaussian likelihood, i.e., we assume that $y=f(x) + e$ where $f$ is the GP, and $e\\sim N(0, \\sigma^2)$ is the Gaussian noise term (referred to as the likelihood in GPy), sometimes called a **nugget** in spatial statistics.\n",
    "2. Specifies an inference scheme, i.e., a way of calculating the posterior distribution of $f$ given the training data. Because we are using a Gaussian likelihood here, the posterior distribution of $f$ is also a Gaussian process, and so inference can be done exactly.\n",
    "\n",
    "We could equivalently set up the model using the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gauss = GPy.likelihoods.Gaussian(variance=1.0)\n",
    "exact = GPy.inference.latent_function_inference.ExactGaussianInference()\n",
    "m1 = GPy.core.GP(X=X, Y=Y, kernel=k, likelihood=gauss, inference_method=exact)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Function Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen during the lectures, the parameters values can be estimated by maximizing the likelihood of the observations. Since we don’t want one of the variance to become negative during the optimization, we can constrain all parameters to be positive before running the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.constrain_positive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warnings are because the parameters are already constrained by default, the software is warning us that they are being reconstrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can optimize the model using the `m.optimize()` method. A number of optimizers are available, such as 'scg', 'lbfgs', 'org-bfgs', 'fmin_tnc', as well as stochastic optimizers if a dependency on a package 'climin' is satisfied, such as 'adadelta' and 'rprop'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.optimize(messages=1, ipython_notebook=True)  # Messages indicates you wish to see the progress of the optimizer, needs ipywidgets to be installed\n",
    "m.plot()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters obtained after optimisation can be compared with the values you selected by hand. As previously, you can modify the kernel used for building the model to investigate its influence on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what has happened outside the range of the data: the GP prediction returns to its prior mean value of 0. We could fix this by adding a prior mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problems with hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, optimization of GP hyper-parameters is not always without problems. The likelihood surface that is being maximized is often flat and multi-modal, and thus the optimizer can sometimes fail to converge, or gets stuck in local-maxima. You can see from the optimization output above, that the gradients at the optima are very small ($10^{-18}$), but not necessarily zero. This kind of problem is due to the use of numerical optimization. When using GPs this can type of problem is common, and so we must always sanity check our answers, and ideally run some diagnostic tests on the fitted model.\n",
    "\n",
    "As an example, consider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "X = np.linspace(0.05,0.95,10)[:,None]\n",
    "Y = -np.cos(np.pi*X) + np.sin(4*np.pi*X) + np.random.normal(loc=0.0, scale=0.1, size=(10,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now fit a GP and optimize the hyper parameters, you may find some problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = GPy.models.GPRegression(X,Y) # this will automatically use a RBF covariance function\n",
    "m.optimize(messages=1)\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "What may have happened here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to fix this is to change the parameter settings so that the optimizer starts in a more sensible location as described above. \n",
    "\n",
    "Another option is to run the optimizer multiple times from multiple different starting points, and pick the best possible (I'd recommend this as good practice). This can be done in GPy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.optimize_restarts(10) # try 10 different starting locations \n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit chosen is the one with the highest maximum likelihood. This is not always what we want, and so we may choose to use methods such as looking at prediction error on a held out dataset, or cross-validation to choose the hyper parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do things like fix certain parameters, constrain them to lie in a given range, and put prior distributions over them. Notice how this information is reported when you type `print(m)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.Gaussian_noise.variance.fix(0.1)\n",
    "m.rbf.lengthscale.constrain_bounded(lower=0.2, upper=1)\n",
    "\n",
    "gamma_prior = GPy.priors.Gamma.from_EV(1.5, 0.7)\n",
    "gamma_prior.plot()\n",
    "m.rbf.variance.set_prior(gamma_prior)\n",
    "m.optimize()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remoe these constraints and priors, we can do something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.unconstrain()\n",
    "m.rbf.variance.unset_priors()\n",
    "print(m)\n",
    "m.constrain_positive() # we've removed the postive constraint above, so we should reset this constraint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "Generate some data from the model $$y = x*sin(x)+e \\qquad e \\sim N(0,0.1^2)$$ for x in the range $[0,5]$. Fit a GP regression model. Do your hyperparameter estimates look reasonable? Try constraining the noise variance to be $0.1^2$. Predict the function in the range $[-5,10]$. Do your predictions look reasonable? What could you do to fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "64beeacf4f284ca08eb087b954a881fb": {
     "views": [
      {
       "cell_index": 74
      }
     ]
    },
    "faee08183a114ae0a6f70c70df457695": {
     "views": [
      {
       "cell_index": 67
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
